{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium\n",
    "!pip install pandas\n",
    "\n",
    "#https://github.com/daneski13/linkedin-job-scraper/blob/main/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run scrapper 1.0\n",
    "#=================================importing necessary Libraries==============================================#\n",
    "import time\n",
    "import pandas as pd    \n",
    "# ------------- # \n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver import ActionChains #to scrolldown \n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "#=================================starting the program=========================================================#\n",
    "start_time = time.strftime(\"%H:%M:%S\", time.localtime())\n",
    "print(\"Starting_Time: \",start_time)\n",
    "\n",
    "#===========================giving User Credentials, roles, location, etc================================#\n",
    "credentialsjson = open(r'D:\\VS_Code\\Pro_1\\linkedincred.json')\n",
    "cred = json.load(credentialsjson)\n",
    "email = cred['email']\n",
    "password = cred['password']\n",
    "credentialsjson.close()\n",
    "\n",
    "position = \"data engineer intern\"\n",
    "location = \"india\"\n",
    "\n",
    "#==================================this function is for starting the page=====================================#\n",
    "def starting_page(email,password,position,location): #returning the driver and no of pages\n",
    "    # Driver path\n",
    "    driver = webdriver.Chrome()  \n",
    "\n",
    "    # Maximize Window\n",
    "    driver.maximize_window() \n",
    "    driver.minimize_window()  \n",
    "    driver.maximize_window()  \n",
    "    driver.switch_to.window(driver.current_window_handle)\n",
    "    driver.implicitly_wait(10)\n",
    "\n",
    "    # Enter to the site\n",
    "    driver.get('https://www.linkedin.com/login');\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Accept cookies\n",
    "    \n",
    "    #==============================finding the elements of username & password and sign in=======================#\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"username\"]').send_keys(email)\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"password\"]').send_keys(password)\n",
    "    time.sleep(1)\n",
    "    # Login button\n",
    "    driver.find_element(By.XPATH,\"//button[@aria-label='Sign in']\").click()\n",
    "    driver.implicitly_wait(10)\n",
    "\n",
    "    #====================================going to the Jobs page==================================================#\n",
    "    driver.get('https://www.linkedin.com/jobs/search')\n",
    "    ## waiting load\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Go to search results directly\n",
    "    #==============fill job title, clearing the location placeholder and fill location and submit================#\n",
    "    driver.find_element(By.XPATH,'//*[@aria-label=\"Search by title, skill, or company\"]').send_keys(position)\n",
    "    driver.find_element(By.XPATH,'//*[@aria-label=\"City, state, or zip code\"]').clear()\n",
    "    driver.find_element(By.XPATH,'//*[@aria-label=\"City, state, or zip code\"]').send_keys(location)\n",
    "    driver.find_element(By.CLASS_NAME,'jobs-search-box__submit-button').click()\n",
    "    time.sleep(30)\n",
    "\n",
    "    ##=============================clicking the date posted buttons and selecting past week=========================##\n",
    "    date_posted_xpath = \"//button[@aria-label='Date posted filter. Clicking this button displays all Date posted filter options.']\"\n",
    "    driver.find_element(By.XPATH,date_posted_xpath).click()\n",
    "    time.sleep(30)\n",
    "    driver.find_element(By.XPATH,\"//span[text()='Past week']\").click()\n",
    "    #=======show results button - multiple show results buttons present - taking the 1st search button for now========#\n",
    "    list_date_modified = driver.find_elements(By.CSS_SELECTOR,'div.reusable-search-filters-buttons>button:nth-child(2)>span')\n",
    "    list_date_modified[0].click()\n",
    "    time.sleep(10)\n",
    "\n",
    "    #==============all buttons in the options - jobs, date posted, experience level, company, on-site remote==============#\n",
    "    all_buttions =driver.find_elements(By.CSS_SELECTOR,\"button.artdeco-pill:nth-child(1)\")\n",
    "    #took all the buttons and selected the 3rd one for experience level\n",
    "\n",
    "    #=====click the experience level and click on entry level, internship and click on show results buttons============#\n",
    "    all_buttions[2].click() #clicking the experience level button\n",
    "    time.sleep(20)\n",
    "    driver.find_element(By.XPATH,\"//span[text()='Internship']\").click()\n",
    "    time.sleep(3)\n",
    "    driver.find_element(By.XPATH,\"//span[text()='Entry level']\").click()\n",
    "    time.sleep(3)\n",
    "    #show results button - multiple show results buttons present - taking the 2nd search button for now\n",
    "    list_date_modified = driver.find_elements(By.CSS_SELECTOR,'div.reusable-search-filters-buttons>button:nth-child(2)>span')\n",
    "    list_date_modified[1].click()\n",
    "    time.sleep(10)\n",
    "    \n",
    "    #==========================getting the total results to calculate the no of pages================================#\n",
    "    total_results_text = driver.find_element(By.CSS_SELECTOR,'div.jobs-search-results-list__subtitle').text\n",
    "    print(total_results_text)\n",
    "    total_results= int(\"\".join(re.findall(r'\\d+', total_results_text)))\n",
    "    \n",
    "    #=======================================calculating the no of pages===============================================#\n",
    "    total_pages = math.ceil(int(total_results)/25)\n",
    "    print(total_pages)\n",
    " \n",
    "    return driver, total_pages #returning driver and total pages\n",
    "\n",
    "functioncall = starting_page(email,password,position,location)\n",
    "driver = functioncall[0]\n",
    "total_pages = functioncall[1]\n",
    "\n",
    "#========================Navigate all pages and collecting all links=============================================#\n",
    "print('Links are being collected now. So go and minimize the screen to collect all links')\n",
    "#=========================screen has to minimized here===============================================#\n",
    "\n",
    "def collect_links(driver,total_pages):#giving driver and total pages\n",
    "\n",
    "    links = []\n",
    "\n",
    "    try: \n",
    "        \n",
    "        #========================going to the Jobs page==================================================#\n",
    "        for page in range(2,total_pages+1):\n",
    "            time.sleep(2)\n",
    "            #==========================identify and get entire jobs links block=================#\n",
    "            jobs_block = driver.find_element(By.CLASS_NAME,'jobs-search-results-list')\n",
    "            \n",
    "            #============================get all job linkselements==============================#\n",
    "            jobs_list= jobs_block.find_elements(By.CLASS_NAME, 'job-card-list__entity-lockup')\n",
    "            \n",
    "            for i in jobs_list:\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView();\", i)\n",
    "                time.sleep(2)\n",
    "            #print(jobs_list)  \n",
    "                jobs_block = driver.find_element(By.CLASS_NAME,'jobs-search-results-list')\n",
    "                jobs_list= jobs_block.find_elements(By.CLASS_NAME, 'job-card-list__entity-lockup')\n",
    "            \n",
    "            #==========================identify and get entire jobs links block=================#\n",
    "            #============================get all job linkselements==============================#\n",
    "            jobs_block = driver.find_element(By.CLASS_NAME,'jobs-search-results-list')\n",
    "            jobs_list= jobs_block.find_elements(By.CLASS_NAME, 'job-card-list__entity-lockup')\n",
    "            \n",
    "            #print(jobs_block.find_elements(By.XPATH,all_links_xpath))\n",
    "            #=====================collect the links one by one===================================#\n",
    "            for job in jobs_list:\n",
    "                all_links = job.find_elements(By.TAG_NAME,'a')\n",
    "                #print(all_links)\n",
    "                for a in all_links:\n",
    "                    if str(a.get_attribute('href')).startswith(\"https://www.linkedin.com/jobs/view\") and a.get_attribute('href') not in links: \n",
    "                        links.append(a.get_attribute('href'))\n",
    "                    else:\n",
    "                        pass\n",
    "                # scroll down for each job element\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView();\", job)\n",
    "                driver.implicitly_wait(10)\n",
    "            \n",
    "            #==========================going to the next page and prining the current time=============#\n",
    "            curr_time = time.strftime(\"%H:%M:%S\", time.localtime())\n",
    "            print(f'Collecting the links in the page: {page-1}',\"Current Time is :\", curr_time)\n",
    "            \n",
    "            #==========================going to 8th page and giving refresh=============================#\n",
    "            if page==8:\n",
    "                driver.refresh()\n",
    "                time.sleep(20)\n",
    "            #===================find the next page element and click to next page=======================#\n",
    "            driver.find_element(By.XPATH,f\"//button[@aria-label='Page {page}']\").click()\n",
    "            time.sleep(3)\n",
    "            \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    driver.close()\n",
    "    \n",
    "    return links\n",
    "\n",
    "links = collect_links(driver,total_pages)\n",
    "\n",
    "print('Found ' + str(len(links)) + ' links for job offers')\n",
    "\n",
    "\n",
    "pd.DataFrame({'Links':links}).to_csv(r'D:\\scrapped_jobs\\Nov5_LinkedinExtractedLinks.csv',index=False)\n",
    "#links = pd.read_csv(r'D:\\scrapped_jobs\\Oct29_LinkedinExtractedLinks.csv').Links.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import time\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "links = pd.read_csv(r'D:\\scrapped_jobs\\Oct29_LinkedinExtractedLinks.csv').Links.to_list()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Scrapper 2.0\n",
    "#==============================creating lists to append all the elements form csv================================#\n",
    "import datetime\n",
    "# for timezone()\n",
    "import pytz\n",
    "Job_Overall_Details = {\"job_title\":[],\"contact_person\":[],\"contact_team\":[],\"company_name\":[],\"company_link\": [],\n",
    "                       \"location_name\":[],\"employmemt_type\": [],\"seniority_level\":[],\"post_date\":[],\n",
    "                       \"applicants_count\":[],\"description\":[],\"job_links\":[]}\n",
    "\n",
    "wait_point = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run scrapper \n",
    "#print(len(job_titles),len(contact_person),len(contact_team),len(company_name),len(company_link),len(location_name),len(employmemt_type),len(seniority_level),len(post_date),len(applicants_count),len(description),len(job_links))\n",
    "print(wait_point)\n",
    "print(links[2])\n",
    "print(Job_Overall_Details[\"job_title\"][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Scrapper 3.0\n",
    "#passing the links and get the details of each job and taking the contacts\n",
    "start_time = time.strftime(\"%H:%M:%S\", time.localtime())\n",
    "print('Start time:', start_time)\n",
    "#\n",
    "def JobDetailedScrapper(links,Job_Overall_Details,wait_point):#giving links and overall details dict and wait point\n",
    "    #==============================collecting the details of each job===========================================#\n",
    "    for link in links:\n",
    "        #======================checking if the job is already scrapped=========================================#\n",
    "        if link not in Job_Overall_Details[\"job_links\"]:\n",
    "            #==============================creating the chrome driver===========================================#\n",
    "            op = webdriver.ChromeOptions()\n",
    "            op.add_argument('headless')\n",
    "            driver = webdriver.Chrome(options=op)\n",
    "            # driver = webdriver.Chrome()\n",
    "            driver.get(link)\n",
    "            driver.maximize_window()\n",
    "            time.sleep(2)\n",
    "            # driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            # time.sleep(1)\n",
    "            #=============================clicking the show more button=========================================#\n",
    "            driver.find_element(By.CLASS_NAME,\"show-more-less-html__button\").click()\n",
    "            #=============================getting the top card job details==========================================#\n",
    "            top_card = driver.find_element(By.CLASS_NAME,'top-card-layout__card')\n",
    "            \n",
    "            #=============================getting the all job details===============================================#\n",
    "            \n",
    "            Job_Overall_Details[\"job_title\"].append(top_card.find_element(By.TAG_NAME,'h1').text)\n",
    "            \n",
    "            #=====================trying to fetch the contact person and contact team if available==================#\n",
    "            try:\n",
    "                person = driver.find_element(By.CSS_SELECTOR,\"div.message-the-recruiter>div>div>h3\").text\n",
    "                team = driver.find_element(By.CSS_SELECTOR,\"div.message-the-recruiter>div>div>h4\").text\n",
    "                print(person,team)\n",
    "                Job_Overall_Details[\"contact_person\"].append(person)\n",
    "                Job_Overall_Details[\"contact_team\"].append(team)\n",
    "            except:\n",
    "                Job_Overall_Details[\"contact_person\"].append('')\n",
    "                Job_Overall_Details[\"contact_team\"].append('')\n",
    "                \n",
    "            #=============================getting the company details===============================================#\n",
    "            Job_Overall_Details[\"company_name\"].append(top_card.find_element(By.CSS_SELECTOR,'div.top-card-layout__entity-info>h4>div>span').text)\n",
    "            Job_Overall_Details[\"company_link\"].append(top_card.find_element(By.TAG_NAME,'a').get_attribute('href'))\n",
    "            Job_Overall_Details[\"location_name\"].append(top_card.find_element(By.CSS_SELECTOR,'div.top-card-layout__entity-info>h4>div>span.topcard__flavor--bullet').text)\n",
    "            \n",
    "            #=============================getting the post date and applicant count=================================#\n",
    "            Job_Overall_Details[\"post_date\"].append(top_card.find_element(By.CLASS_NAME,'posted-time-ago__text').text)\n",
    "            Job_Overall_Details[\"applicants_count\"].append(top_card.find_element(By.CLASS_NAME,'num-applicants__caption').text)\n",
    "            \n",
    "            #=============================getting the seniority level and employment type============================#\n",
    "            job_criteria_text_list = [i.text for i in driver.find_elements(By.CSS_SELECTOR,\"span.description__job-criteria-text\")]\n",
    "            \n",
    "            #employment type\n",
    "            if len(job_criteria_text_list) >= 2:\n",
    "                Job_Overall_Details[\"employmemt_type\"].append(job_criteria_text_list[1])\n",
    "            else:\n",
    "                Job_Overall_Details[\"employmemt_type\"].append('None')\n",
    "            \n",
    "            Job_Overall_Details[\"seniority_level\"].append(job_criteria_text_list[0])\n",
    "            \n",
    "            #=============================getting the job description and job links==================================#\n",
    "            #print(driver.find_element(By.CLASS_NAME,'description__text').text)\n",
    "            Job_Overall_Details[\"description\"].append(driver.find_element(By.CLASS_NAME,'description__text').text)\n",
    "            Job_Overall_Details[\"job_links\"].append(link)\n",
    "            \n",
    "            #===================================closing the driver==================================================#\n",
    "            driver.close()\n",
    "\n",
    "            wait_point += 1\n",
    "            #=================================if 15 jobs scrapped then sleep for 10 sec===========================#\n",
    "            if wait_point % 15 == 0:\n",
    "                \n",
    "                #using now() to get current time\n",
    "                current_time = time.strftime(\"%H:%M:%S\", time.localtime())\n",
    "                print(f\"{wait_point} links completed\",\" The current time in india is :\", current_time)\n",
    "                time.sleep(10)\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            pass\n",
    "    \n",
    "    return Job_Overall_Details\n",
    "\n",
    "#=============================calling the function===============================================# \n",
    "JobScrapperReturn = JobDetailedScrapper(links,Job_Overall_Details,wait_point)       \n",
    "df = pd.DataFrame(JobScrapperReturn)\n",
    "file_location = f\"D:\\\\scrapped_jobs\\\\Nov5_{'_'.join(position.split())}_linkedin_jobs.csv\"\n",
    "df.to_csv(file_location,index=False)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(JobScrapperReturn)\n",
    "\n",
    "df.to_csv(r'D:\\scrapped_jobs\\Oct22_data_engineer_intern_intern_22ndOCT_22_linkedin_jobs.csv',index=False)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run scrapper 4.0\n",
    "#merging all the datasets and getting only contact\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "#function to read all csv files \n",
    "def read_file(): #reading files inside the folders\n",
    "    \n",
    "    csv_path = r'D:\\scrapped_jobs\\alldata' #\n",
    "\n",
    "    # filenames = glob.glob(path + \"\\*.xlsx\")\n",
    "    path_filenames = glob. glob(csv_path + \"/*.csv\")\n",
    "    path_filenames.sort()\n",
    "#     print('File names:', csv_filenames)\n",
    "    \n",
    "    return path_filenames\n",
    "\n",
    "AllFiles=read_file()\n",
    "\n",
    "def file_names(path_filenames): #getting file names\n",
    "    files = []\n",
    "    \n",
    "    for index,file in enumerate(path_filenames):\n",
    "        f = file.split(\"D:\\\\scrapped_jobs\\\\alldata\\\\\")[1].split(\".csv\")[0]\n",
    "        files.append(f'{f}')\n",
    "\n",
    "    return files\n",
    "\n",
    "files = file_names(read_file())\n",
    "\n",
    "merged_frame = pd.DataFrame({'job_title':[], 'contact_person':[], 'company_name':[],'job_links':[],'scrap_date':[]})\n",
    "\n",
    "for i,j in zip(AllFiles,files):\n",
    "    \n",
    "    month_list = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n",
    "    \n",
    "    df = pd.read_csv(i) #creating dataframe by reading csv files\n",
    "    \n",
    "    #finding the scrap date and adding it to the dataframe\n",
    "    scrap_day = j.split('_')[0]\n",
    "    if scrap_day[0:3] in month_list:\n",
    "        \n",
    "        scrap_date = datetime.datetime(2023, month_list.index(scrap_day[0:3])+1, int(scrap_day[3:]))\n",
    "        df['scrap_date'] = scrap_date\n",
    "        columns_to_take = ['job_title', 'contact_person', 'company_name','job_links','scrap_date']\n",
    "        try:\n",
    "            df = df.loc[:,columns_to_take]\n",
    "            bool_index = df.contact_person.apply(lambda x: type(x).__name__) != 'float'\n",
    "            index_list = [j for j,i in enumerate(bool_index) if i == True]\n",
    "            print(scrap_day,index_list)\n",
    "            df = df.iloc[index_list,[0,1,2,3,4]]\n",
    "            merged_frame = pd.concat([merged_frame,df],ignore_index=True)\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "merged_frame.sort_values('scrap_date',inplace=True)\n",
    "merged_frame.to_csv(r'D:\\scrapped_jobs\\LinkedinContactsData.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_frame.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
