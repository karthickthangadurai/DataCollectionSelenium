{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in d:\\vs_code\\pro_1\\.venv\\lib\\site-packages (4.14.0)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in d:\\vs_code\\pro_1\\.venv\\lib\\site-packages (from selenium) (2.0.6)\n",
      "Requirement already satisfied: trio~=0.17 in d:\\vs_code\\pro_1\\.venv\\lib\\site-packages (from selenium) (0.22.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in d:\\vs_code\\pro_1\\.venv\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in d:\\vs_code\\pro_1\\.venv\\lib\\site-packages (from selenium) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=20.1.0 in d:\\vs_code\\pro_1\\.venv\\lib\\site-packages (from trio~=0.17->selenium) (23.1.0)\n",
      "Requirement already satisfied: sortedcontainers in d:\\vs_code\\pro_1\\.venv\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in d:\\vs_code\\pro_1\\.venv\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in d:\\vs_code\\pro_1\\.venv\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sniffio in d:\\vs_code\\pro_1\\.venv\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in d:\\vs_code\\pro_1\\.venv\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in d:\\vs_code\\pro_1\\.venv\\lib\\site-packages (from trio~=0.17->selenium) (1.1.3)\n",
      "Requirement already satisfied: wsproto>=0.14 in d:\\vs_code\\pro_1\\.venv\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in d:\\vs_code\\pro_1\\.venv\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in d:\\vs_code\\pro_1\\.venv\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in d:\\vs_code\\pro_1\\.venv\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Collecting pandas\n",
      "  Obtaining dependency information for pandas from https://files.pythonhosted.org/packages/ce/cd/a7c2cbffe2afff975349e60b14b63a448162145a7acac8ba12ddc2ed78a8/pandas-2.1.1-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading pandas-2.1.1-cp310-cp310-win_amd64.whl.metadata (18 kB)\n",
      "Collecting numpy>=1.22.4 (from pandas)\n",
      "  Obtaining dependency information for numpy>=1.22.4 from https://files.pythonhosted.org/packages/cc/05/ef9fc04adda45d537619ea956bc33489f50a46badc949c4280d8309185ec/numpy-1.26.0-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading numpy-1.26.0-cp310-cp310-win_amd64.whl.metadata (61 kB)\n",
      "     ---------------------------------------- 0.0/61.1 kB ? eta -:--:--\n",
      "     ------------------------- ------------ 41.0/61.1 kB 991.0 kB/s eta 0:00:01\n",
      "     ---------------------------------------- 61.1/61.1 kB 1.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\vs_code\\pro_1\\.venv\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Obtaining dependency information for pytz>=2020.1 from https://files.pythonhosted.org/packages/32/4d/aaf7eff5deb402fd9a24a1449a8119f00d74ae9c2efa79f8ef9994261fc2/pytz-2023.3.post1-py2.py3-none-any.whl.metadata\n",
      "  Downloading pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas)\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Requirement already satisfied: six>=1.5 in d:\\vs_code\\pro_1\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.1.1-cp310-cp310-win_amd64.whl (10.7 MB)\n",
      "   ---------------------------------------- 0.0/10.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/10.7 MB 5.5 MB/s eta 0:00:02\n",
      "    --------------------------------------- 0.2/10.7 MB 2.1 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.3/10.7 MB 3.4 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.5/10.7 MB 2.6 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 0.6/10.7 MB 2.8 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.8/10.7 MB 2.9 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 1.0/10.7 MB 3.2 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.2/10.7 MB 3.3 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.3/10.7 MB 3.4 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.6/10.7 MB 3.4 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.8/10.7 MB 3.5 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 1.9/10.7 MB 3.6 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 2.1/10.7 MB 3.7 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.3/10.7 MB 3.7 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.5/10.7 MB 3.7 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.7/10.7 MB 3.8 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.9/10.7 MB 3.8 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.2/10.7 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.3/10.7 MB 3.8 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.6/10.7 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.7/10.7 MB 3.8 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 3.9/10.7 MB 3.8 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.0/10.7 MB 3.8 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.0/10.7 MB 3.8 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.0/10.7 MB 3.8 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 4.5/10.7 MB 3.9 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 4.7/10.7 MB 3.8 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.8/10.7 MB 3.8 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 5.1/10.7 MB 3.8 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 5.3/10.7 MB 3.8 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 5.5/10.7 MB 3.8 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 5.7/10.7 MB 3.8 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 5.7/10.7 MB 3.7 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 6.0/10.7 MB 3.8 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 6.2/10.7 MB 3.8 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 6.4/10.7 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 6.6/10.7 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 6.9/10.7 MB 3.9 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.1/10.7 MB 3.9 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.4/10.7 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 7.6/10.7 MB 4.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.0/10.7 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.0/10.7 MB 4.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 8.3/10.7 MB 4.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 8.4/10.7 MB 4.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.6/10.7 MB 4.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.8/10.7 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.1/10.7 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.3/10.7 MB 4.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.5/10.7 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.7/10.7 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.0/10.7 MB 4.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.3/10.7 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.5/10.7 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.7/10.7 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.7/10.7 MB 4.1 MB/s eta 0:00:00\n",
      "Downloading numpy-1.26.0-cp310-cp310-win_amd64.whl (15.8 MB)\n",
      "   ---------------------------------------- 0.0/15.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.4/15.8 MB 11.2 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 0.6/15.8 MB 7.4 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.8/15.8 MB 5.8 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 1.0/15.8 MB 5.4 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 1.2/15.8 MB 5.6 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 1.4/15.8 MB 5.2 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.7/15.8 MB 5.3 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.9/15.8 MB 5.1 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 2.1/15.8 MB 4.9 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 2.4/15.8 MB 5.1 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 2.5/15.8 MB 4.7 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 2.8/15.8 MB 4.8 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 3.1/15.8 MB 4.9 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 3.3/15.8 MB 4.9 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 3.4/15.8 MB 4.7 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 3.7/15.8 MB 4.8 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 3.9/15.8 MB 4.7 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 4.2/15.8 MB 4.8 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 4.3/15.8 MB 4.8 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 4.6/15.8 MB 4.7 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 4.8/15.8 MB 4.7 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 5.1/15.8 MB 4.8 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 5.4/15.8 MB 4.8 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 5.7/15.8 MB 4.9 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 5.9/15.8 MB 4.7 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 6.2/15.8 MB 4.8 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 6.3/15.8 MB 4.7 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 6.6/15.8 MB 4.8 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 6.7/15.8 MB 4.8 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 6.8/15.8 MB 4.7 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 7.1/15.8 MB 4.7 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 7.5/15.8 MB 4.8 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 7.7/15.8 MB 4.8 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 8.0/15.8 MB 4.8 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 8.3/15.8 MB 4.8 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 8.6/15.8 MB 4.8 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 8.8/15.8 MB 4.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 9.0/15.8 MB 4.8 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 9.3/15.8 MB 4.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 9.4/15.8 MB 4.8 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 9.8/15.8 MB 4.8 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 10.1/15.8 MB 4.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 10.3/15.8 MB 4.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 10.6/15.8 MB 4.8 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 10.8/15.8 MB 4.8 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 11.0/15.8 MB 4.7 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 11.2/15.8 MB 4.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 11.5/15.8 MB 4.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 11.7/15.8 MB 4.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 11.9/15.8 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 12.3/15.8 MB 5.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 12.6/15.8 MB 5.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 13.0/15.8 MB 5.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 13.3/15.8 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 13.5/15.8 MB 5.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 14.0/15.8 MB 5.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 14.0/15.8 MB 5.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 14.4/15.8 MB 5.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 14.6/15.8 MB 5.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 15.0/15.8 MB 5.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.2/15.8 MB 5.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.6/15.8 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.8/15.8 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.8/15.8 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.8/15.8 MB 5.2 MB/s eta 0:00:00\n",
      "Downloading pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "   ---------------------------------------- 0.0/502.5 kB ? eta -:--:--\n",
      "   --------------------------------- ----- 430.1/502.5 kB 13.5 MB/s eta 0:00:01\n",
      "   --------------------------------------- 502.5/502.5 kB 10.5 MB/s eta 0:00:00\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-1.26.0 pandas-2.1.1 pytz-2023.3.post1 tzdata-2023.3\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting_Time:  15:57:43\n",
      "687 results\n",
      "28\n",
      "Links are being collected now.\n",
      "Collecting the links in the page: 1 Current Time is : 16:00:52\n",
      "Collecting the links in the page: 2 Current Time is : 16:01:32\n",
      "Found 50 links for job offers\n"
     ]
    }
   ],
   "source": [
    "#=================================importing necessary Libraries==============================================#\n",
    "import time\n",
    "import pandas as pd    \n",
    "# ------------- # \n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver import ActionChains #to scrolldown \n",
    "import math\n",
    "import re\n",
    "import time\n",
    "\n",
    "#=================================starting the program=========================================================#\n",
    "start_time = time.strftime(\"%H:%M:%S\", time.localtime())\n",
    "print(\"Starting_Time: \",start_time)\n",
    "\n",
    "#===========================giving User Credentials, roles, location, etc================================#\n",
    "email = \"email_@outlook.com\"\n",
    "password = \"Password\"\n",
    "position = \"data engineer intern\"\n",
    "location = \"india\"\n",
    "#==================================this function is for starting the page=====================================#\n",
    "def starting_page(email,password,position,location): #returning the driver and no of pages\n",
    "    # Driver path\n",
    "    driver = webdriver.Chrome()  \n",
    "\n",
    "    # Maximize Window\n",
    "    driver.maximize_window() \n",
    "    driver.minimize_window()  \n",
    "    driver.maximize_window()  \n",
    "    driver.switch_to.window(driver.current_window_handle)\n",
    "    driver.implicitly_wait(10)\n",
    "\n",
    "    # Enter to the site\n",
    "    driver.get('https://www.linkedin.com/login');\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Accept cookies\n",
    "    \n",
    "    #==============================finding the elements of username & password and sign in=======================#\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"username\"]').send_keys(email)\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"password\"]').send_keys(password)\n",
    "    time.sleep(1)\n",
    "    # Login button\n",
    "    driver.find_element(By.XPATH,\"//button[@aria-label='Sign in']\").click()\n",
    "    driver.implicitly_wait(10)\n",
    "\n",
    "    #====================================going to the Jobs page==================================================#\n",
    "    driver.get('https://www.linkedin.com/jobs/search')\n",
    "    ## waiting load\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Go to search results directly\n",
    "    #==============fill job title, clearing the location placeholder and fill location and submit================#\n",
    "    driver.find_element(By.XPATH,'//*[@aria-label=\"Search by title, skill, or company\"]').send_keys(position)\n",
    "    driver.find_element(By.XPATH,'//*[@aria-label=\"City, state, or zip code\"]').clear()\n",
    "    driver.find_element(By.XPATH,'//*[@aria-label=\"City, state, or zip code\"]').send_keys(location)\n",
    "    driver.find_element(By.CLASS_NAME,'jobs-search-box__submit-button').click()\n",
    "    time.sleep(30)\n",
    "\n",
    "    ##=============================clicking the date posted buttons and selecting past week=========================##\n",
    "    date_posted_xpath = \"//button[@aria-label='Date posted filter. Clicking this button displays all Date posted filter options.']\"\n",
    "    driver.find_element(By.XPATH,date_posted_xpath).click()\n",
    "    time.sleep(30)\n",
    "    driver.find_element(By.XPATH,\"//span[text()='Past week']\").click()\n",
    "    #=======show results button - multiple show results buttons present - taking the 1st search button for now========#\n",
    "    list_date_modified = driver.find_elements(By.CSS_SELECTOR,'div.reusable-search-filters-buttons>button:nth-child(2)>span')\n",
    "    list_date_modified[0].click()\n",
    "    time.sleep(10)\n",
    "\n",
    "    #==============all buttons in the options - jobs, date posted, experience level, company, on-site remote==============#\n",
    "    all_buttions =driver.find_elements(By.CSS_SELECTOR,\"button.artdeco-pill:nth-child(1)\")\n",
    "    #took all the buttons and selected the 3rd one for experience level\n",
    "\n",
    "    #=====click the experience level and click on entry level, internship and click on show results buttons============#\n",
    "    all_buttions[2].click() #clicking the experience level button\n",
    "    time.sleep(20)\n",
    "    driver.find_element(By.XPATH,\"//span[text()='Internship']\").click()\n",
    "    time.sleep(3)\n",
    "    driver.find_element(By.XPATH,\"//span[text()='Entry level']\").click()\n",
    "    time.sleep(3)\n",
    "    #show results button - multiple show results buttons present - taking the 2nd search button for now\n",
    "    list_date_modified = driver.find_elements(By.CSS_SELECTOR,'div.reusable-search-filters-buttons>button:nth-child(2)>span')\n",
    "    list_date_modified[1].click()\n",
    "    time.sleep(10)\n",
    "    \n",
    "    #==========================getting the total results to calculate the no of pages================================#\n",
    "    total_results_text = driver.find_element(By.CSS_SELECTOR,'div.jobs-search-results-list__subtitle').text\n",
    "    print(total_results_text)\n",
    "    total_results= int(\"\".join(re.findall(r'\\d+', total_results_text)))\n",
    "    \n",
    "    #=======================================calculating the no of pages===============================================#\n",
    "    total_pages = math.ceil(int(total_results)/25)\n",
    "    print(total_pages)\n",
    " \n",
    "    return driver, total_pages #returning driver and total pages\n",
    "\n",
    "functioncall = starting_page(email,password,position,location)\n",
    "driver = functioncall[0]\n",
    "total_pages = functioncall[1]\n",
    "\n",
    "#========================Navigate all pages and collecting all links=============================================#\n",
    "print('Links are being collected now. So go and minimize the screen to collect all links')\n",
    "#=========================screen has to minimized here===============================================#\n",
    "\n",
    "def collect_links(driver,total_pages):#giving driver and total pages\n",
    "\n",
    "    links = []\n",
    "\n",
    "    try: \n",
    "        \n",
    "        #========================going to the Jobs page==================================================#\n",
    "        for page in range(2,total_pages+1):\n",
    "            time.sleep(2)\n",
    "            #==========================identify and get entire jobs links block=================#\n",
    "            jobs_block = driver.find_element(By.CLASS_NAME,'jobs-search-results-list')\n",
    "            \n",
    "            #============================get all job linkselements==============================#\n",
    "            jobs_list= jobs_block.find_elements(By.CLASS_NAME, 'job-card-list__entity-lockup')\n",
    "            \n",
    "            for i in jobs_list:\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView();\", i)\n",
    "                time.sleep(2)\n",
    "            #print(jobs_list)  \n",
    "                jobs_block = driver.find_element(By.CLASS_NAME,'jobs-search-results-list')\n",
    "                jobs_list= jobs_block.find_elements(By.CLASS_NAME, 'job-card-list__entity-lockup')\n",
    "            \n",
    "            #==========================identify and get entire jobs links block=================#\n",
    "            #============================get all job linkselements==============================#\n",
    "            jobs_block = driver.find_element(By.CLASS_NAME,'jobs-search-results-list')\n",
    "            jobs_list= jobs_block.find_elements(By.CLASS_NAME, 'job-card-list__entity-lockup')\n",
    "            \n",
    "            #print(jobs_block.find_elements(By.XPATH,all_links_xpath))\n",
    "            #=====================collect the links one by one===================================#\n",
    "            for job in jobs_list:\n",
    "                all_links = job.find_elements(By.TAG_NAME,'a')\n",
    "                #print(all_links)\n",
    "                for a in all_links:\n",
    "                    if str(a.get_attribute('href')).startswith(\"https://www.linkedin.com/jobs/view\") and a.get_attribute('href') not in links: \n",
    "                        links.append(a.get_attribute('href'))\n",
    "                    else:\n",
    "                        pass\n",
    "                # scroll down for each job element\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView();\", job)\n",
    "                driver.implicitly_wait(10)\n",
    "            \n",
    "            #==========================going to the next page and prining the current time=============#\n",
    "            curr_time = time.strftime(\"%H:%M:%S\", time.localtime())\n",
    "            print(f'Collecting the links in the page: {page-1}',\"Current Time is :\", curr_time)\n",
    "            \n",
    "            #==========================going to 8th page and giving refresh=============================#\n",
    "            if page==8:\n",
    "                driver.refresh()\n",
    "                time.sleep(20)\n",
    "            #===================find the next page element and click to next page=======================#\n",
    "            driver.find_element(By.XPATH,f\"//button[@aria-label='Page {page}']\").click()\n",
    "            time.sleep(3)\n",
    "            \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return links\n",
    "\n",
    "links = collect_links(driver,total_pages)\n",
    "\n",
    "print('Found ' + str(len(links)) + ' links for job offers')\n",
    "driver.close()\n",
    "\n",
    "pd.DataFrame({'Links':links}).to_csv(r'D:\\scrapped_jobs\\Oct22_LinkedinExtractedLinks.csv',index=False)\n",
    "links = pd.read_csv(r'D:\\scrapped_jobs\\Oct22_LinkedinExtractedLinks.csv').Links.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================creating lists to append all the elements form csv================================#\n",
    "import datetime\n",
    "# for timezone()\n",
    "import pytz\n",
    "Job_Overall_Details = {\"job_titles\":[],\"contact_person\":[],\"contact_team\":[],\"company_name\":[],\"company_link\": [],\n",
    "                       \"location_name\":[],\"employmemt_type\": [],\"seniority_level\":[],\"post_date\":[],\n",
    "                       \"applicants_count\":[],\"description\":[],\"job_links\":[]}\n",
    "\n",
    "wait_point = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "https://www.linkedin.com/jobs/view/3728816636/?eBP=JOB_SEARCH_ORGANIC&refId=vig3LjUN0urbHtcobLSQPA%3D%3D&trackingId=SQH7ZoOXQoJkOV6NBPfSbA%3D%3D&trk=flagship3_search_srp_jobs\n",
      "Hadoop Cluster Developer\n"
     ]
    }
   ],
   "source": [
    "#print(len(job_titles),len(contact_person),len(contact_team),len(company_name),len(company_link),len(location_name),len(employmemt_type),len(seniority_level),len(post_date),len(applicants_count),len(description),len(job_links))\n",
    "print(wait_point)\n",
    "print(links[4])\n",
    "print(Job_Overall_Details[\"job_titles\"][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 17:00:14\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_titles</th>\n",
       "      <th>contact_person</th>\n",
       "      <th>contact_team</th>\n",
       "      <th>company_name</th>\n",
       "      <th>company_link</th>\n",
       "      <th>location_name</th>\n",
       "      <th>employmemt_type</th>\n",
       "      <th>seniority_level</th>\n",
       "      <th>post_date</th>\n",
       "      <th>applicants_count</th>\n",
       "      <th>description</th>\n",
       "      <th>job_links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Statistical Programmer</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Pfizer</td>\n",
       "      <td>https://www.linkedin.com/company/pfizer?trk=pu...</td>\n",
       "      <td>Chennai, Tamil Nadu, India</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>46 minutes ago</td>\n",
       "      <td>Over 200 applicants</td>\n",
       "      <td>Job Summary:\\n\\nThis role is a hands on Statis...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3693559357/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Statistical Programmer</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Pfizer</td>\n",
       "      <td>https://www.linkedin.com/company/pfizer?trk=pu...</td>\n",
       "      <td>Chennai, Tamil Nadu, India</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>47 minutes ago</td>\n",
       "      <td>Over 200 applicants</td>\n",
       "      <td>Job Summary:\\n\\nThis role is a hands on Statis...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3693559357/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Outsystems Development</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>UST</td>\n",
       "      <td>https://www.linkedin.com/company/ustglobal?trk...</td>\n",
       "      <td>Kochi, Kerala, India</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>6 days ago</td>\n",
       "      <td>29 applicants</td>\n",
       "      <td>Job Description\\n\\nRole Proficiency:\\n\\nAct cr...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3741790299/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Big Data Developer</td>\n",
       "      <td>Subhashis Sahoo</td>\n",
       "      <td>Director at Avihs</td>\n",
       "      <td>Avihs</td>\n",
       "      <td>https://in.linkedin.com/company/www.avihs.com?...</td>\n",
       "      <td>Bengaluru, Karnataka, India</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>5 days ago</td>\n",
       "      <td>74 applicants</td>\n",
       "      <td>We are seeking a skilled Big Data Engineer / A...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3736279833/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hadoop Cluster Developer</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Infosys</td>\n",
       "      <td>https://in.linkedin.com/company/infosys?trk=pu...</td>\n",
       "      <td>Trivandrum, Kerala, India</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>2 days ago</td>\n",
       "      <td>50 applicants</td>\n",
       "      <td>Primary skills:Analytics-&gt;Cluster Analysis,App...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3730473721/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 job_titles   contact_person       contact_team company_name  \\\n",
       "0    Statistical Programmer                                           Pfizer   \n",
       "1    Statistical Programmer                                           Pfizer   \n",
       "2    Outsystems Development                                              UST   \n",
       "3        Big Data Developer  Subhashis Sahoo  Director at Avihs        Avihs   \n",
       "4  Hadoop Cluster Developer                                          Infosys   \n",
       "\n",
       "                                        company_link  \\\n",
       "0  https://www.linkedin.com/company/pfizer?trk=pu...   \n",
       "1  https://www.linkedin.com/company/pfizer?trk=pu...   \n",
       "2  https://www.linkedin.com/company/ustglobal?trk...   \n",
       "3  https://in.linkedin.com/company/www.avihs.com?...   \n",
       "4  https://in.linkedin.com/company/infosys?trk=pu...   \n",
       "\n",
       "                 location_name employmemt_type seniority_level  \\\n",
       "0   Chennai, Tamil Nadu, India       Full-time     Entry level   \n",
       "1   Chennai, Tamil Nadu, India       Full-time     Entry level   \n",
       "2         Kochi, Kerala, India       Full-time     Entry level   \n",
       "3  Bengaluru, Karnataka, India       Full-time     Entry level   \n",
       "4    Trivandrum, Kerala, India       Full-time     Entry level   \n",
       "\n",
       "        post_date     applicants_count  \\\n",
       "0  46 minutes ago  Over 200 applicants   \n",
       "1  47 minutes ago  Over 200 applicants   \n",
       "2      6 days ago        29 applicants   \n",
       "3      5 days ago        74 applicants   \n",
       "4      2 days ago        50 applicants   \n",
       "\n",
       "                                         description  \\\n",
       "0  Job Summary:\\n\\nThis role is a hands on Statis...   \n",
       "1  Job Summary:\\n\\nThis role is a hands on Statis...   \n",
       "2  Job Description\\n\\nRole Proficiency:\\n\\nAct cr...   \n",
       "3  We are seeking a skilled Big Data Engineer / A...   \n",
       "4  Primary skills:Analytics->Cluster Analysis,App...   \n",
       "\n",
       "                                           job_links  \n",
       "0  https://www.linkedin.com/jobs/view/3693559357/...  \n",
       "1  https://www.linkedin.com/jobs/view/3693559357/...  \n",
       "2  https://www.linkedin.com/jobs/view/3741790299/...  \n",
       "3  https://www.linkedin.com/jobs/view/3736279833/...  \n",
       "4  https://www.linkedin.com/jobs/view/3730473721/...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#passing the links and get the details of each job and taking the contacts\n",
    "start_time = time.strftime(\"%H:%M:%S\", time.localtime())\n",
    "print('Start time:', start_time)\n",
    "#\n",
    "def JobDetailedScrapper(links,Job_Overall_Details,wait_point):#giving links and overall details dict and wait point\n",
    "    #==============================collecting the details of each job===========================================#\n",
    "    for link in links:\n",
    "        #======================checking if the job is already scrapped=========================================#\n",
    "        if link not in Job_Overall_Details[\"job_links\"]:\n",
    "            #==============================creating the chrome driver===========================================#\n",
    "            op = webdriver.ChromeOptions()\n",
    "            op.add_argument('headless')\n",
    "            driver = webdriver.Chrome(options=op)\n",
    "            # driver = webdriver.Chrome()\n",
    "            driver.get(link)\n",
    "            driver.maximize_window()\n",
    "            time.sleep(2)\n",
    "            # driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            # time.sleep(1)\n",
    "            #=============================clicking the show more button=========================================#\n",
    "            driver.find_element(By.CLASS_NAME,\"show-more-less-html__button\").click()\n",
    "            #=============================getting the top card job details==========================================#\n",
    "            top_card = driver.find_element(By.CLASS_NAME,'top-card-layout__card')\n",
    "            \n",
    "            #=============================getting the all job details===============================================#\n",
    "            \n",
    "            Job_Overall_Details[\"job_titles\"].append(top_card.find_element(By.TAG_NAME,'h1').text)\n",
    "            \n",
    "            #=====================trying to fetch the contact person and contact team if available==================#\n",
    "            try:\n",
    "                person = driver.find_element(By.CSS_SELECTOR,\"div.message-the-recruiter>div>div>h3\").text\n",
    "                team = driver.find_element(By.CSS_SELECTOR,\"div.message-the-recruiter>div>div>h4\").text\n",
    "                print(person,team)\n",
    "                Job_Overall_Details[\"contact_person\"].append(person)\n",
    "                Job_Overall_Details[\"contact_team\"].append(team)\n",
    "            except:\n",
    "                Job_Overall_Details[\"contact_person\"].append('')\n",
    "                Job_Overall_Details[\"contact_team\"].append('')\n",
    "                \n",
    "            #=============================getting the company details===============================================#\n",
    "            Job_Overall_Details[\"company_name\"].append(top_card.find_element(By.CSS_SELECTOR,'div.top-card-layout__entity-info>h4>div>span').text)\n",
    "            Job_Overall_Details[\"company_link\"].append(top_card.find_element(By.TAG_NAME,'a').get_attribute('href'))\n",
    "            Job_Overall_Details[\"location_name\"].append(top_card.find_element(By.CSS_SELECTOR,'div.top-card-layout__entity-info>h4>div>span.topcard__flavor--bullet').text)\n",
    "            \n",
    "            #=============================getting the post date and applicant count=================================#\n",
    "            Job_Overall_Details[\"post_date\"].append(top_card.find_element(By.CLASS_NAME,'posted-time-ago__text').text)\n",
    "            Job_Overall_Details[\"applicants_count\"].append(top_card.find_element(By.CLASS_NAME,'num-applicants__caption').text)\n",
    "            \n",
    "            #=============================getting the seniority level and employment type============================#\n",
    "            job_criteria_text_list = [i.text for i in driver.find_elements(By.CSS_SELECTOR,\"span.description__job-criteria-text\")]\n",
    "            \n",
    "            #employment type\n",
    "            if len(job_criteria_text_list) >= 2:\n",
    "                Job_Overall_Details[\"employmemt_type\"].append(job_criteria_text_list[1])\n",
    "            else:\n",
    "                Job_Overall_Details[\"employmemt_type\"].append('None')\n",
    "            \n",
    "            Job_Overall_Details[\"seniority_level\"].append(job_criteria_text_list[0])\n",
    "            \n",
    "            #=============================getting the job description and job links==================================#\n",
    "            #print(driver.find_element(By.CLASS_NAME,'description__text').text)\n",
    "            Job_Overall_Details[\"description\"].append(driver.find_element(By.CLASS_NAME,'description__text').text)\n",
    "            Job_Overall_Details[\"job_links\"].append(link)\n",
    "            \n",
    "            #===================================closing the driver==================================================#\n",
    "            driver.close()\n",
    "\n",
    "            wait_point += 1\n",
    "            #=================================if 15 jobs scrapped then sleep for 10 sec===========================#\n",
    "            if wait_point % 15 == 0:\n",
    "                \n",
    "                #using now() to get current time\n",
    "                current_time = time.strftime(\"%H:%M:%S\", time.localtime())\n",
    "                print(f\"{wait_point} links completed\",\" The current time in india is :\", current_time)\n",
    "                time.sleep(10)\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            pass\n",
    "    \n",
    "    return Job_Overall_Details\n",
    "\n",
    "#=============================calling the function===============================================# \n",
    "JobScrapperReturn = JobDetailedScrapper(links,Job_Overall_Details,wait_point)       \n",
    "df = pd.DataFrame(JobScrapperReturn)\n",
    "\n",
    "df.to_csv(r'D:\\scrapped_jobs\\Oct22_data_engineer_intern_linkedin_jobs.csv',index=False)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>contact_person</th>\n",
       "      <th>company_name</th>\n",
       "      <th>company_link</th>\n",
       "      <th>location_name</th>\n",
       "      <th>post_dates</th>\n",
       "      <th>applicants_count</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>seniority_level</th>\n",
       "      <th>description</th>\n",
       "      <th>Job_links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Analyst Data Science Analytics</td>\n",
       "      <td></td>\n",
       "      <td>ClickJobs.io</td>\n",
       "      <td>https://uk.linkedin.com/company/clickjobsio?tr...</td>\n",
       "      <td>Bengaluru, Karnataka, India</td>\n",
       "      <td>5 hours ago</td>\n",
       "      <td>103 applicants</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>Grow your career with a growing organization\\n...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3738351411/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst (Mutual Series)</td>\n",
       "      <td></td>\n",
       "      <td>Franklin Templeton India</td>\n",
       "      <td>https://in.linkedin.com/company/franklintemple...</td>\n",
       "      <td>Mumbai, Maharashtra, India</td>\n",
       "      <td>2 days ago</td>\n",
       "      <td>Over 200 applicants</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>At Franklin Templeton, everything we do is foc...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3737214595/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Process Developer – Data Analyst-COR021664</td>\n",
       "      <td></td>\n",
       "      <td>Genpact</td>\n",
       "      <td>https://www.linkedin.com/company/genpact?trk=p...</td>\n",
       "      <td>Noida, Uttar Pradesh, India</td>\n",
       "      <td>4 days ago</td>\n",
       "      <td>Over 200 applicants</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>With a startup spirit and 90,000+ curious and ...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3623351494/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Forecasting Analyst</td>\n",
       "      <td></td>\n",
       "      <td>NewAge Products Inc.</td>\n",
       "      <td>https://ca.linkedin.com/company/newage-product...</td>\n",
       "      <td>India</td>\n",
       "      <td>1 day ago</td>\n",
       "      <td>Over 200 applicants</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>NewAge Products Inc. (NAP), headquartered in N...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3737546187/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Internal Audit - Data Strategy - Analyst - Hyd...</td>\n",
       "      <td></td>\n",
       "      <td>Goldman Sachs</td>\n",
       "      <td>https://www.linkedin.com/company/goldman-sachs...</td>\n",
       "      <td>Hyderabad, Telangana, India</td>\n",
       "      <td>4 hours ago</td>\n",
       "      <td>Over 200 applicants</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>Job Description\\n\\nWhat We Do\\n\\nInternal Audi...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3648716404/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           job_title contact_person  \\\n",
       "0                     Analyst Data Science Analytics                  \n",
       "1                       Data Analyst (Mutual Series)                  \n",
       "2         Process Developer – Data Analyst-COR021664                  \n",
       "3                                Forecasting Analyst                  \n",
       "4  Internal Audit - Data Strategy - Analyst - Hyd...                  \n",
       "\n",
       "               company_name  \\\n",
       "0              ClickJobs.io   \n",
       "1  Franklin Templeton India   \n",
       "2                   Genpact   \n",
       "3      NewAge Products Inc.   \n",
       "4             Goldman Sachs   \n",
       "\n",
       "                                        company_link  \\\n",
       "0  https://uk.linkedin.com/company/clickjobsio?tr...   \n",
       "1  https://in.linkedin.com/company/franklintemple...   \n",
       "2  https://www.linkedin.com/company/genpact?trk=p...   \n",
       "3  https://ca.linkedin.com/company/newage-product...   \n",
       "4  https://www.linkedin.com/company/goldman-sachs...   \n",
       "\n",
       "                 location_name   post_dates     applicants_count  \\\n",
       "0  Bengaluru, Karnataka, India  5 hours ago       103 applicants   \n",
       "1   Mumbai, Maharashtra, India   2 days ago  Over 200 applicants   \n",
       "2  Noida, Uttar Pradesh, India   4 days ago  Over 200 applicants   \n",
       "3                        India    1 day ago  Over 200 applicants   \n",
       "4  Hyderabad, Telangana, India  4 hours ago  Over 200 applicants   \n",
       "\n",
       "  employment_type seniority_level  \\\n",
       "0       Full-time     Entry level   \n",
       "1       Full-time     Entry level   \n",
       "2       Full-time     Entry level   \n",
       "3       Full-time     Entry level   \n",
       "4       Full-time     Entry level   \n",
       "\n",
       "                                         description  \\\n",
       "0  Grow your career with a growing organization\\n...   \n",
       "1  At Franklin Templeton, everything we do is foc...   \n",
       "2  With a startup spirit and 90,000+ curious and ...   \n",
       "3  NewAge Products Inc. (NAP), headquartered in N...   \n",
       "4  Job Description\\n\\nWhat We Do\\n\\nInternal Audi...   \n",
       "\n",
       "                                           Job_links  \n",
       "0  https://www.linkedin.com/jobs/view/3738351411/...  \n",
       "1  https://www.linkedin.com/jobs/view/3737214595/...  \n",
       "2  https://www.linkedin.com/jobs/view/3623351494/...  \n",
       "3  https://www.linkedin.com/jobs/view/3737546187/...  \n",
       "4  https://www.linkedin.com/jobs/view/3648716404/...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(JobScrapperReturn)\n",
    "\n",
    "df.to_csv(r'D:\\scrapped_jobs\\Oct22_data_engineer_intern_intern_22ndOCT_22_linkedin_jobs.csv',index=False)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging all the datasets and getting only contact\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "#function to read all csv files \n",
    "def read_file(): #reading files inside the folders\n",
    "    \n",
    "    csv_path = r'D:\\scrapped_jobs' #\n",
    "\n",
    "    # filenames = glob.glob(path + \"\\*.xlsx\")\n",
    "    path_filenames = glob. glob(csv_path + \"/*.csv\")\n",
    "    path_filenames.sort()\n",
    "#     print('File names:', csv_filenames)\n",
    "    \n",
    "    return path_filenames\n",
    "\n",
    "AllFiles=read_file()\n",
    "\n",
    "def file_names(path_filenames): #getting file names\n",
    "    files = []\n",
    "    \n",
    "    for index,file in enumerate(path_filenames):\n",
    "        f = file.split(\"D:\\\\scrapped_jobs\\\\\")[1].split(\".csv\")[0]\n",
    "        files.append(f'{f}')\n",
    "\n",
    "    return files\n",
    "\n",
    "files = file_names(read_file())\n",
    "\n",
    "merged_frame = pd.DataFrame({'job_title':[], 'contact_person':[], 'company_name':[],'Job_links':[],'scrap_date':[]})\n",
    "\n",
    "for i,j in zip(AllFiles,files):\n",
    "    \n",
    "    month_list = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n",
    "    \n",
    "    df = pd.read_csv(i) #creating dataframe by reading csv files\n",
    "    \n",
    "    #finding the scrap date and adding it to the dataframe\n",
    "    scrap_day = j.split('_')[0]\n",
    "    if scrap_day[0:3] in month_list:\n",
    "        scrap_date = datetime.datetime(2023, month_list.index(scrap_day[0:3])+1, int(scrap_day[3:]))\n",
    "        df['scrap_date'] = scrap_date\n",
    "        columns_to_take = ['job_title', 'contact_person', 'company_name','Job_links','scrap_date']\n",
    "    \n",
    "        try:\n",
    "            df = df.loc[:,columns_to_take]\n",
    "            bool_index = df.contact_person.apply(lambda x: type(x).__name__) != 'float'\n",
    "            index_list = [j for j,i in enumerate(bool_index) if i == True]\n",
    "            df = df.iloc[index_list,[0,1,2,3,4]]\n",
    "            merged_frame = pd.concat([merged_frame,df],ignore_index=True)\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "merged_frame.sort_values('scrap_date',inplace=True)\n",
    "merged_frame.to_csv(r'D:\\scrapped_jobs\\LinkedinContactsData.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
