{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in d:\\vs_code\\pro_1\\.venv\\lib\\site-packages (4.14.0)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in d:\\vs_code\\pro_1\\.venv\\lib\\site-packages (from selenium) (2.0.6)\n",
      "Requirement already satisfied: trio~=0.17 in d:\\vs_code\\pro_1\\.venv\\lib\\site-packages (from selenium) (0.22.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in d:\\vs_code\\pro_1\\.venv\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in d:\\vs_code\\pro_1\\.venv\\lib\\site-packages (from selenium) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=20.1.0 in d:\\vs_code\\pro_1\\.venv\\lib\\site-packages (from trio~=0.17->selenium) (23.1.0)\n",
      "Requirement already satisfied: sortedcontainers in d:\\vs_code\\pro_1\\.venv\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in d:\\vs_code\\pro_1\\.venv\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in d:\\vs_code\\pro_1\\.venv\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sniffio in d:\\vs_code\\pro_1\\.venv\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in d:\\vs_code\\pro_1\\.venv\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in d:\\vs_code\\pro_1\\.venv\\lib\\site-packages (from trio~=0.17->selenium) (1.1.3)\n",
      "Requirement already satisfied: wsproto>=0.14 in d:\\vs_code\\pro_1\\.venv\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in d:\\vs_code\\pro_1\\.venv\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in d:\\vs_code\\pro_1\\.venv\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in d:\\vs_code\\pro_1\\.venv\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Collecting pandas\n",
      "  Obtaining dependency information for pandas from https://files.pythonhosted.org/packages/ce/cd/a7c2cbffe2afff975349e60b14b63a448162145a7acac8ba12ddc2ed78a8/pandas-2.1.1-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading pandas-2.1.1-cp310-cp310-win_amd64.whl.metadata (18 kB)\n",
      "Collecting numpy>=1.22.4 (from pandas)\n",
      "  Obtaining dependency information for numpy>=1.22.4 from https://files.pythonhosted.org/packages/cc/05/ef9fc04adda45d537619ea956bc33489f50a46badc949c4280d8309185ec/numpy-1.26.0-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading numpy-1.26.0-cp310-cp310-win_amd64.whl.metadata (61 kB)\n",
      "     ---------------------------------------- 0.0/61.1 kB ? eta -:--:--\n",
      "     ------------------------- ------------ 41.0/61.1 kB 991.0 kB/s eta 0:00:01\n",
      "     ---------------------------------------- 61.1/61.1 kB 1.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\vs_code\\pro_1\\.venv\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Obtaining dependency information for pytz>=2020.1 from https://files.pythonhosted.org/packages/32/4d/aaf7eff5deb402fd9a24a1449a8119f00d74ae9c2efa79f8ef9994261fc2/pytz-2023.3.post1-py2.py3-none-any.whl.metadata\n",
      "  Downloading pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas)\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Requirement already satisfied: six>=1.5 in d:\\vs_code\\pro_1\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.1.1-cp310-cp310-win_amd64.whl (10.7 MB)\n",
      "   ---------------------------------------- 0.0/10.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/10.7 MB 5.5 MB/s eta 0:00:02\n",
      "    --------------------------------------- 0.2/10.7 MB 2.1 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.3/10.7 MB 3.4 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.5/10.7 MB 2.6 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 0.6/10.7 MB 2.8 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.8/10.7 MB 2.9 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 1.0/10.7 MB 3.2 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.2/10.7 MB 3.3 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.3/10.7 MB 3.4 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.6/10.7 MB 3.4 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.8/10.7 MB 3.5 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 1.9/10.7 MB 3.6 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 2.1/10.7 MB 3.7 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.3/10.7 MB 3.7 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.5/10.7 MB 3.7 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.7/10.7 MB 3.8 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.9/10.7 MB 3.8 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.2/10.7 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.3/10.7 MB 3.8 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.6/10.7 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.7/10.7 MB 3.8 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 3.9/10.7 MB 3.8 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.0/10.7 MB 3.8 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.0/10.7 MB 3.8 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.0/10.7 MB 3.8 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 4.5/10.7 MB 3.9 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 4.7/10.7 MB 3.8 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.8/10.7 MB 3.8 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 5.1/10.7 MB 3.8 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 5.3/10.7 MB 3.8 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 5.5/10.7 MB 3.8 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 5.7/10.7 MB 3.8 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 5.7/10.7 MB 3.7 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 6.0/10.7 MB 3.8 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 6.2/10.7 MB 3.8 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 6.4/10.7 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 6.6/10.7 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 6.9/10.7 MB 3.9 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.1/10.7 MB 3.9 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.4/10.7 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 7.6/10.7 MB 4.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.0/10.7 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.0/10.7 MB 4.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 8.3/10.7 MB 4.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 8.4/10.7 MB 4.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.6/10.7 MB 4.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.8/10.7 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.1/10.7 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.3/10.7 MB 4.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.5/10.7 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.7/10.7 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.0/10.7 MB 4.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.3/10.7 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.5/10.7 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.7/10.7 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.7/10.7 MB 4.1 MB/s eta 0:00:00\n",
      "Downloading numpy-1.26.0-cp310-cp310-win_amd64.whl (15.8 MB)\n",
      "   ---------------------------------------- 0.0/15.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.4/15.8 MB 11.2 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 0.6/15.8 MB 7.4 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.8/15.8 MB 5.8 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 1.0/15.8 MB 5.4 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 1.2/15.8 MB 5.6 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 1.4/15.8 MB 5.2 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.7/15.8 MB 5.3 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.9/15.8 MB 5.1 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 2.1/15.8 MB 4.9 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 2.4/15.8 MB 5.1 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 2.5/15.8 MB 4.7 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 2.8/15.8 MB 4.8 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 3.1/15.8 MB 4.9 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 3.3/15.8 MB 4.9 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 3.4/15.8 MB 4.7 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 3.7/15.8 MB 4.8 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 3.9/15.8 MB 4.7 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 4.2/15.8 MB 4.8 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 4.3/15.8 MB 4.8 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 4.6/15.8 MB 4.7 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 4.8/15.8 MB 4.7 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 5.1/15.8 MB 4.8 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 5.4/15.8 MB 4.8 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 5.7/15.8 MB 4.9 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 5.9/15.8 MB 4.7 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 6.2/15.8 MB 4.8 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 6.3/15.8 MB 4.7 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 6.6/15.8 MB 4.8 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 6.7/15.8 MB 4.8 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 6.8/15.8 MB 4.7 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 7.1/15.8 MB 4.7 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 7.5/15.8 MB 4.8 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 7.7/15.8 MB 4.8 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 8.0/15.8 MB 4.8 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 8.3/15.8 MB 4.8 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 8.6/15.8 MB 4.8 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 8.8/15.8 MB 4.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 9.0/15.8 MB 4.8 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 9.3/15.8 MB 4.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 9.4/15.8 MB 4.8 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 9.8/15.8 MB 4.8 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 10.1/15.8 MB 4.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 10.3/15.8 MB 4.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 10.6/15.8 MB 4.8 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 10.8/15.8 MB 4.8 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 11.0/15.8 MB 4.7 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 11.2/15.8 MB 4.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 11.5/15.8 MB 4.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 11.7/15.8 MB 4.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 11.9/15.8 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 12.3/15.8 MB 5.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 12.6/15.8 MB 5.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 13.0/15.8 MB 5.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 13.3/15.8 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 13.5/15.8 MB 5.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 14.0/15.8 MB 5.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 14.0/15.8 MB 5.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 14.4/15.8 MB 5.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 14.6/15.8 MB 5.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 15.0/15.8 MB 5.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.2/15.8 MB 5.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.6/15.8 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.8/15.8 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.8/15.8 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.8/15.8 MB 5.2 MB/s eta 0:00:00\n",
      "Downloading pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "   ---------------------------------------- 0.0/502.5 kB ? eta -:--:--\n",
      "   --------------------------------- ----- 430.1/502.5 kB 13.5 MB/s eta 0:00:01\n",
      "   --------------------------------------- 502.5/502.5 kB 10.5 MB/s eta 0:00:00\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-1.26.0 pandas-2.1.1 pytz-2023.3.post1 tzdata-2023.3\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting_Time:  19:11:22\n",
      "482 results\n",
      "20\n",
      "Links are being collected now.\n",
      "Collecting the links in the page: 1 Current Time is : 19:14:25\n",
      "Collecting the links in the page: 2 Current Time is : 19:15:05\n",
      "Collecting the links in the page: 3 Current Time is : 19:15:42\n",
      "Collecting the links in the page: 4 Current Time is : 19:16:21\n",
      "Collecting the links in the page: 5 Current Time is : 19:17:01\n",
      "Collecting the links in the page: 6 Current Time is : 19:17:40\n",
      "Collecting the links in the page: 7 Current Time is : 19:18:20\n",
      "Collecting the links in the page: 8 Current Time is : 19:19:22\n",
      "Collecting the links in the page: 9 Current Time is : 19:19:59\n",
      "Collecting the links in the page: 10 Current Time is : 19:20:42\n",
      "Collecting the links in the page: 11 Current Time is : 19:21:21\n",
      "Collecting the links in the page: 12 Current Time is : 19:22:02\n",
      "Collecting the links in the page: 13 Current Time is : 19:22:24\n",
      "Collecting the links in the page: 14 Current Time is : 19:22:45\n",
      "Collecting the links in the page: 15 Current Time is : 19:23:06\n",
      "Collecting the links in the page: 16 Current Time is : 19:23:28\n",
      "Collecting the links in the page: 17 Current Time is : 19:23:50\n",
      "Collecting the links in the page: 18 Current Time is : 19:24:12\n",
      "Collecting the links in the page: 19 Current Time is : 19:24:33\n",
      "Found 340 links for job offers\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import time\n",
    "import pandas as pd    \n",
    "# ------------- # \n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver import ActionChains #to scrolldown \n",
    "import math\n",
    "import re\n",
    "import time\n",
    "\n",
    "start_time = time.strftime(\"%H:%M:%S\", time.localtime())\n",
    "print(\"Starting_Time: \",start_time)\n",
    "\n",
    "def starting_page():\n",
    "    # Driver path\n",
    "    driver = webdriver.Chrome()  \n",
    "\n",
    "    # Maximize Window\n",
    "    driver.maximize_window() \n",
    "    driver.minimize_window()  \n",
    "    driver.maximize_window()  \n",
    "    driver.switch_to.window(driver.current_window_handle)\n",
    "    driver.implicitly_wait(10)\n",
    "\n",
    "    # Enter to the site\n",
    "    driver.get('https://www.linkedin.com/login');\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Accept cookies\n",
    "\n",
    "    # User Credentials\n",
    "    email = \"email_name@outlook.com\"\n",
    "    password = \"password\"\n",
    "    position = \"data engineer intern\"\n",
    "    local = \"india\"\n",
    "\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"username\"]').send_keys(email)\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"password\"]').send_keys(password)\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Login button\n",
    "    driver.find_element(By.XPATH,\"//button[@aria-label='Sign in']\").click()\n",
    "    driver.implicitly_wait(10)\n",
    "\n",
    "    # Jobs page\n",
    "    driver.get('https://www.linkedin.com/jobs/search')\n",
    "    ## waiting load\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Go to search results directly\n",
    "    # driver.get(\"https://www.linkedin.com/jobs/search/?geoId=105646813&keywords=junior%20data%20analyst&location=Chennai\")\n",
    "    driver.find_element(By.XPATH,'//*[@aria-label=\"Search by title, skill, or company\"]').send_keys(position)\n",
    "    driver.find_element(By.XPATH,'//*[@aria-label=\"City, state, or zip code\"]').clear()\n",
    "    driver.find_element(By.XPATH,'//*[@aria-label=\"City, state, or zip code\"]').send_keys(local)\n",
    "    driver.find_element(By.CLASS_NAME,'jobs-search-box__submit-button').click()\n",
    "    time.sleep(30)\n",
    "\n",
    "    #date posted buttons - past week\n",
    "    date_posted_xpath = \"//button[@aria-label='Date posted filter. Clicking this button displays all Date posted filter options.']\"\n",
    "    driver.find_element(By.XPATH,date_posted_xpath).click()\n",
    "    time.sleep(30)\n",
    "    driver.find_element(By.XPATH,\"//span[text()='Past week']\").click()\n",
    "    #show results button - multiple show results buttons present - taking the 1st search button for now\n",
    "    list_date_modified = driver.find_elements(By.CSS_SELECTOR,'div.reusable-search-filters-buttons>button:nth-child(2)>span')\n",
    "    list_date_modified[0].click()\n",
    "    time.sleep(10)\n",
    "\n",
    "    #all buttons - jobs, date posted, experience level, company, on-site remote\n",
    "    all_buttions =driver.find_elements(By.CSS_SELECTOR,\"button.artdeco-pill:nth-child(1)\")\n",
    "    #took all the buttons and selected the 3rd one for experience level\n",
    "\n",
    "    #experience level\n",
    "    all_buttions[2].click() #clicking the experience level button\n",
    "    time.sleep(20)\n",
    "    driver.find_element(By.XPATH,\"//span[text()='Internship']\").click()\n",
    "    time.sleep(3)\n",
    "    driver.find_element(By.XPATH,\"//span[text()='Entry level']\").click()\n",
    "    time.sleep(3)\n",
    "    #show results button - multiple show results buttons present - taking the 2nd search button for now\n",
    "    list_date_modified = driver.find_elements(By.CSS_SELECTOR,'div.reusable-search-filters-buttons>button:nth-child(2)>span')\n",
    "    list_date_modified[1].click()\n",
    "    time.sleep(10)\n",
    "\n",
    "    #total results to calculate the no of pages\n",
    "    total_results_text = driver.find_element(By.CSS_SELECTOR,'div.jobs-search-results-list__subtitle').text\n",
    "    print(total_results_text)\n",
    "    total_results= int(\"\".join(re.findall(r'\\d+', total_results_text)))\n",
    "    total_pages = math.ceil(int(total_results)/25)\n",
    "    print(total_pages)\n",
    "    # Get all links for these offers\n",
    "\n",
    "    return driver, total_pages\n",
    "\n",
    "functioncall = starting_page()\n",
    "driver = functioncall[0]\n",
    "total_pages = functioncall[1]\n",
    "\n",
    "# Navigate 13 pages\n",
    "print('Links are being collected now.')\n",
    "links = []\n",
    "job_titles_list = []\n",
    "company_names_list = []\n",
    "location_names_list = []\n",
    "posted_dates_list = []\n",
    "applicants_count_list = []\n",
    "\n",
    "\n",
    "try: \n",
    "    for page in range(2,total_pages+1):\n",
    "        time.sleep(2)\n",
    "\n",
    "        #identify and get entire jobs block \n",
    "        jobs_block = driver.find_element(By.CLASS_NAME,'jobs-search-results-list')\n",
    "        \n",
    "        #get all job elements\n",
    "        jobs_list= jobs_block.find_elements(By.CLASS_NAME, 'job-card-list__entity-lockup')\n",
    "        \n",
    "        for i in jobs_list:\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView();\", i)\n",
    "            time.sleep(2)\n",
    "        #print(jobs_list)  \n",
    "            jobs_block = driver.find_element(By.CLASS_NAME,'jobs-search-results-list')\n",
    "            jobs_list= jobs_block.find_elements(By.CLASS_NAME, 'job-card-list__entity-lockup')\n",
    "        \n",
    "        jobs_block = driver.find_element(By.CLASS_NAME,'jobs-search-results-list')\n",
    "        jobs_list= jobs_block.find_elements(By.CLASS_NAME, 'job-card-list__entity-lockup')\n",
    "        \n",
    "        \n",
    "        #print(jobs_block.find_elements(By.XPATH,all_links_xpath))\n",
    "        \n",
    "        for job in jobs_list:\n",
    "            all_links = job.find_elements(By.TAG_NAME,'a')\n",
    "            #print(all_links)\n",
    "            for a in all_links:\n",
    "                if str(a.get_attribute('href')).startswith(\"https://www.linkedin.com/jobs/view\") and a.get_attribute('href') not in links: \n",
    "                    links.append(a.get_attribute('href'))\n",
    "                else:\n",
    "                    pass\n",
    "            # scroll down for each job element\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView();\", job)\n",
    "            driver.implicitly_wait(10)\n",
    "        \n",
    "        curr_time = time.strftime(\"%H:%M:%S\", time.localtime())\n",
    "        print(f'Collecting the links in the page: {page-1}',\"Current Time is :\", curr_time)\n",
    "        \n",
    "        # time.sleep(20)\n",
    "        if page==8:\n",
    "            driver.refresh()\n",
    "            time.sleep(20)\n",
    "        # go to next page:\n",
    "        driver.find_element(By.XPATH,f\"//button[@aria-label='Page {page}']\").click()\n",
    "        time.sleep(3)\n",
    "        \n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "print('Found ' + str(len(links)) + ' links for job offers')\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Links':links}).to_csv(r'D:\\scrapped_jobs\\Oct22_LinkedinExtractedLinks.csv',index=False)\n",
    "links = pd.read_csv(r'D:\\scrapped_jobs\\Oct22_LinkedinExtractedLinks.csv').Links.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#form csv \n",
    "import datetime\n",
    "# for timezone()\n",
    "import pytz\n",
    "job_titles = []\n",
    "contact_person = []\n",
    "contact_team = []\n",
    "company_name =[]\n",
    "company_link = []\n",
    "location_name = []\n",
    "employmemt_type = []\n",
    "seniority_level = []\n",
    "post_date = []\n",
    "applicants_count = []\n",
    "description = []\n",
    "job_links = []\n",
    "\n",
    "wait_point = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "print(len(job_titles),len(contact_person),len(contact_team),len(company_name),len(company_link),len(location_name),len(employmemt_type),len(seniority_level),len(post_date),len(applicants_count),len(description),len(job_links))\n",
    "print(wait_point)\n",
    "print(links[28])\n",
    "print(job_titles[27])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 19:35:59\n",
      "15 links completed  The current time in india is : 19:37:14\n",
      "30 links completed  The current time in india is : 19:39:33\n",
      "Lalit Shukla Innovator | Quantum Computing | Deep neural networks\n",
      "45 links completed  The current time in india is : 19:41:58\n",
      "Kamini Kumari Singh Talent Acquisition Executive I - US IT Recruiter at Codvo.ai Hiring for US IT Roles..C2C/W2/USC/GC/H4 and for India AWS Devops and Data Scientist\n",
      "Prashant Kumar Technical Recruiter at IntraEdge\n",
      "Muhammed Irfan Ali C T Director at Webcoffee Innovations | Engineering, Design, Development\n",
      "60 links completed  The current time in india is : 19:44:22\n",
      "75 links completed  The current time in india is : 19:46:43\n",
      "90 links completed  The current time in india is : 19:49:04\n",
      "Priyanka Meena Talent Hire Specialist, PeopleSoft, D365,DevOps, SAP, ServiceNow,Big Data, Python,Java, Cypress QA etc...\n",
      "105 links completed  The current time in india is : 19:51:27\n",
      "Garima Shukla Human resource\n",
      "120 links completed  The current time in india is : 19:53:48\n",
      "Shravan Kumar Senior Talent Acquisition\n",
      "135 links completed  The current time in india is : 19:56:09\n",
      "150 links completed  The current time in india is : 19:58:29\n",
      "Sukanya S Talent Acquisition Specialist at Kaseya\n",
      "Pooja Pawar Talent Advisor at Sattva Human\n",
      "165 links completed  The current time in india is : 20:00:52\n",
      "Daniel Hyde Global Tech Senior Recruiter at Alter Domus\n",
      "180 links completed  The current time in india is : 20:03:14\n",
      "195 links completed  The current time in india is : 20:05:33\n",
      "Vishvajeet Suryawanshi Client Services and Delivery Lead\n",
      "210 links completed  The current time in india is : 20:07:50\n",
      "Bindiya Singh MSP - Program Specialist at Dell\n",
      "225 links completed  The current time in india is : 20:10:08\n",
      "Vishvajeet Suryawanshi Client Services and Delivery Lead\n",
      "240 links completed  The current time in india is : 20:12:27\n",
      "SARATH KRISHNA Senior Candidate Manager. (Organisational/Industrial Psychologist)\n",
      "255 links completed  The current time in india is : 20:14:44\n",
      "270 links completed  The current time in india is : 20:17:02\n",
      "285 links completed  The current time in india is : 20:19:22\n",
      "Rajesh Sathuluri Senior Technical Recruiter @ Fisker Inc | Talent Acquisition Specialist\n",
      "300 links completed  The current time in india is : 20:21:41\n",
      "Aruna Arikati Hiring SRE / DevOps Engineer - PragmaticPlay\n",
      "315 links completed  The current time in india is : 20:23:59\n",
      "330 links completed  The current time in india is : 20:26:15\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>contact_person</th>\n",
       "      <th>company_name</th>\n",
       "      <th>company_link</th>\n",
       "      <th>location_name</th>\n",
       "      <th>post_dates</th>\n",
       "      <th>applicants_count</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>seniority_level</th>\n",
       "      <th>description</th>\n",
       "      <th>Job_links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hadoop Cluster Developer</td>\n",
       "      <td></td>\n",
       "      <td>Infosys</td>\n",
       "      <td>https://in.linkedin.com/company/infosys?trk=pu...</td>\n",
       "      <td>Trivandrum, Kerala, India</td>\n",
       "      <td>2 days ago</td>\n",
       "      <td>52 applicants</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>Experience in Hadoop cluster management – Incl...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3730472762/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Microsoft Engineer</td>\n",
       "      <td></td>\n",
       "      <td>NTT Ltd.</td>\n",
       "      <td>https://uk.linkedin.com/company/global-ntt?trk...</td>\n",
       "      <td>Hyderabad, Telangana, India</td>\n",
       "      <td>2 days ago</td>\n",
       "      <td>125 applicants</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>Want to be a part of our team?\\n\\nAbout us:\\n\\...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3730211764/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Clinical Data Science Programmer</td>\n",
       "      <td></td>\n",
       "      <td>ICON plc</td>\n",
       "      <td>https://ie.linkedin.com/company/icon-plc-2?trk...</td>\n",
       "      <td>Bengaluru, Karnataka, India</td>\n",
       "      <td>5 days ago</td>\n",
       "      <td>171 applicants</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>ICON plc is a world-leading healthcare intelli...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3741898600/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Outsystems Development</td>\n",
       "      <td></td>\n",
       "      <td>UST</td>\n",
       "      <td>https://www.linkedin.com/company/ustglobal?trk...</td>\n",
       "      <td>Kochi, Kerala, India</td>\n",
       "      <td>6 days ago</td>\n",
       "      <td>25 applicants</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>Job Description\\n\\nRole Proficiency:\\n\\nAct cr...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3741790299/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FinOps Data Analytics Engineer</td>\n",
       "      <td></td>\n",
       "      <td>WPP</td>\n",
       "      <td>https://uk.linkedin.com/company/wpp?trk=public...</td>\n",
       "      <td>Chennai, Tamil Nadu, India</td>\n",
       "      <td>3 days ago</td>\n",
       "      <td>124 applicants</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>WPP is the creative transformation company. We...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3728816636/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          job_title contact_person company_name  \\\n",
       "0          Hadoop Cluster Developer                     Infosys   \n",
       "1                Microsoft Engineer                    NTT Ltd.   \n",
       "2  Clinical Data Science Programmer                    ICON plc   \n",
       "3            Outsystems Development                         UST   \n",
       "4    FinOps Data Analytics Engineer                         WPP   \n",
       "\n",
       "                                        company_link  \\\n",
       "0  https://in.linkedin.com/company/infosys?trk=pu...   \n",
       "1  https://uk.linkedin.com/company/global-ntt?trk...   \n",
       "2  https://ie.linkedin.com/company/icon-plc-2?trk...   \n",
       "3  https://www.linkedin.com/company/ustglobal?trk...   \n",
       "4  https://uk.linkedin.com/company/wpp?trk=public...   \n",
       "\n",
       "                 location_name  post_dates applicants_count employment_type  \\\n",
       "0    Trivandrum, Kerala, India  2 days ago    52 applicants       Full-time   \n",
       "1  Hyderabad, Telangana, India  2 days ago   125 applicants       Full-time   \n",
       "2  Bengaluru, Karnataka, India  5 days ago   171 applicants       Full-time   \n",
       "3         Kochi, Kerala, India  6 days ago    25 applicants       Full-time   \n",
       "4   Chennai, Tamil Nadu, India  3 days ago   124 applicants       Full-time   \n",
       "\n",
       "  seniority_level                                        description  \\\n",
       "0     Entry level  Experience in Hadoop cluster management – Incl...   \n",
       "1     Entry level  Want to be a part of our team?\\n\\nAbout us:\\n\\...   \n",
       "2     Entry level  ICON plc is a world-leading healthcare intelli...   \n",
       "3     Entry level  Job Description\\n\\nRole Proficiency:\\n\\nAct cr...   \n",
       "4     Entry level  WPP is the creative transformation company. We...   \n",
       "\n",
       "                                           Job_links  \n",
       "0  https://www.linkedin.com/jobs/view/3730472762/...  \n",
       "1  https://www.linkedin.com/jobs/view/3730211764/...  \n",
       "2  https://www.linkedin.com/jobs/view/3741898600/...  \n",
       "3  https://www.linkedin.com/jobs/view/3741790299/...  \n",
       "4  https://www.linkedin.com/jobs/view/3728816636/...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#passing the links and get the details of each job and taking the contacts\n",
    "start_time = time.strftime(\"%H:%M:%S\", time.localtime())\n",
    "print('Start time:', start_time)\n",
    "for link in links:\n",
    "    \n",
    "    if link not in job_links:\n",
    "        op = webdriver.ChromeOptions()\n",
    "        op.add_argument('headless')\n",
    "        driver = webdriver.Chrome(options=op)\n",
    "        # driver = webdriver.Chrome()\n",
    "        #link = 'https://www.linkedin.com/jobs/view/3686754061/?eBP=CwEAAAGKxxnSTkzu5AJfFnurpPVatjpllHHB32r-ddM530uibLmc_RchDB6r_7r49B-A0SpNg1NqRKoXbXEXVTGdl9B_cg4jq9HmhNYOCEZejWlxlEazXsQqFxha6p7WBo5JbgJKRfuRk8OOTQJZbUPyIAom2baGvyjrnAl3p_IUF2U3dxIVHgqZzSJScdQWyzQmMoGW1XtS6AOEybaJeroFuLf74lX0ifI4te5r7poxBhxyjYI6rad0N4zOe-brnZd2v_emaBfnbFvfueOH4KhV2DaJk8vNNg5GK-HeXhMaSo0wFiKhl-wEPjfKzts9G-Tho9DmrD1EN1BGu7LmfQJV-97Sw3Fx98jsk7Xmedsk0xWIjI5uGs5zl97sUVSwMj1IafYhWwY&refId=ceCqJZR9Mvb7jxY3ND9zqA%3D%3D&trackingId=yrX7jnEhAflhLz6e%2BXkACQ%3D%3D&trk=flagship3_search_srp_jobs'\n",
    "        driver.get(link)\n",
    "        driver.maximize_window()\n",
    "        time.sleep(2)\n",
    "        # driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        # time.sleep(1)\n",
    "        driver.find_element(By.CLASS_NAME,\"show-more-less-html__button\").click()\n",
    "        \n",
    "        top_card = driver.find_element(By.CLASS_NAME,'top-card-layout__card')\n",
    "        #print('job_title-->',top_card.find_element(By.TAG_NAME,'h1').text)\n",
    "        job_titles.append(top_card.find_element(By.TAG_NAME,'h1').text)\n",
    "        \n",
    "        try:\n",
    "            person = driver.find_element(By.CSS_SELECTOR,\"div.message-the-recruiter>div>div>h3\").text\n",
    "            team = driver.find_element(By.CSS_SELECTOR,\"div.message-the-recruiter>div>div>h4\").text\n",
    "            print(person,team)\n",
    "            contact_person.append(person)\n",
    "            contact_team.append(team)\n",
    "        except:\n",
    "            contact_person.append('')\n",
    "            contact_team.append('')\n",
    "        \n",
    "        #print('company_name-->',top_card.find_element(By.XPATH,'//*[@id=\"main-content\"]/section[1]/div/section[2]/div/div[1]/div/h4/div[1]/span[1]/a').text)\n",
    "        company_name.append(top_card.find_element(By.CSS_SELECTOR,'div.top-card-layout__entity-info>h4>div>span').text)\n",
    "        \n",
    "        #print('company_link-->',top_card.find_element(By.TAG_NAME,'a').get_attribute('href'))\n",
    "        company_link.append(top_card.find_element(By.TAG_NAME,'a').get_attribute('href'))\n",
    "        \n",
    "        #print('location-->',top_card.find_element(By.XPATH,'//*[@id=\"main-content\"]/section[1]/div/section[2]/div/div[1]/div/h4/div[1]/span[2]').text)\n",
    "        location_name.append(top_card.find_element(By.CSS_SELECTOR,'div.top-card-layout__entity-info>h4>div>span.topcard__flavor--bullet').text)\n",
    "        \n",
    "        #print('post_dates-->',top_card.find_element(By.CLASS_NAME,'posted-time-ago__text').text)\n",
    "        post_date.append(top_card.find_element(By.CLASS_NAME,'posted-time-ago__text').text)\n",
    "        \n",
    "        #print('applicants_count-->',top_card.find_element(By.CLASS_NAME,'num-applicants__caption').text)\n",
    "        applicants_count.append(top_card.find_element(By.CLASS_NAME,'num-applicants__caption').text)\n",
    "        \n",
    "        job_criteria_text_list = [i.text for i in driver.find_elements(By.CSS_SELECTOR,\"span.description__job-criteria-text\")]\n",
    "        \n",
    "        #employment type\n",
    "        if len(job_criteria_text_list) >= 2:\n",
    "            employmemt_type.append(job_criteria_text_list[1])\n",
    "        else:\n",
    "            employmemt_type.append('None')\n",
    "        #print('seniority_level-->',top_card.find_element(By.XPATH,'//*[@id=\"main-content\"]/section[1]/div/div/section[1]/div/ul/li[1]/span').text)\n",
    "        seniority_level.append(job_criteria_text_list[0])\n",
    "        \n",
    "        #print(driver.find_element(By.CLASS_NAME,'description__text').text)\n",
    "        description.append(driver.find_element(By.CLASS_NAME,'description__text').text)\n",
    "        job_links.append(link)\n",
    "        \n",
    "        driver.close()\n",
    "\n",
    "        wait_point += 1\n",
    "        \n",
    "        if wait_point % 15 == 0:\n",
    "            \n",
    "            # using now() to get current time\n",
    "            current_time = time.strftime(\"%H:%M:%S\", time.localtime())\n",
    "            print(f\"{wait_point} links completed\",\" The current time in india is :\", current_time)\n",
    "            \n",
    "            time.sleep(10)\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        pass\n",
    "        \n",
    "df = pd.DataFrame({\n",
    "    'job_title':job_titles,'contact_person':contact_person,'company_name':company_name,'company_link':company_link,\n",
    "    'location_name':location_name,'post_dates':post_date,\n",
    "    'applicants_count':applicants_count,'employment_type':employmemt_type,\n",
    "    'seniority_level':seniority_level,'description':description,'Job_links':job_links\n",
    "})\n",
    "\n",
    "df.to_csv(r'D:\\scrapped_jobs\\Oct22_data_engineer_intern_linkedin_jobs.csv',index=False)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>contact_person</th>\n",
       "      <th>company_name</th>\n",
       "      <th>company_link</th>\n",
       "      <th>location_name</th>\n",
       "      <th>post_dates</th>\n",
       "      <th>applicants_count</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>seniority_level</th>\n",
       "      <th>description</th>\n",
       "      <th>Job_links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Analyst Data Science Analytics</td>\n",
       "      <td></td>\n",
       "      <td>ClickJobs.io</td>\n",
       "      <td>https://uk.linkedin.com/company/clickjobsio?tr...</td>\n",
       "      <td>Bengaluru, Karnataka, India</td>\n",
       "      <td>5 hours ago</td>\n",
       "      <td>103 applicants</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>Grow your career with a growing organization\\n...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3738351411/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst (Mutual Series)</td>\n",
       "      <td></td>\n",
       "      <td>Franklin Templeton India</td>\n",
       "      <td>https://in.linkedin.com/company/franklintemple...</td>\n",
       "      <td>Mumbai, Maharashtra, India</td>\n",
       "      <td>2 days ago</td>\n",
       "      <td>Over 200 applicants</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>At Franklin Templeton, everything we do is foc...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3737214595/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Process Developer – Data Analyst-COR021664</td>\n",
       "      <td></td>\n",
       "      <td>Genpact</td>\n",
       "      <td>https://www.linkedin.com/company/genpact?trk=p...</td>\n",
       "      <td>Noida, Uttar Pradesh, India</td>\n",
       "      <td>4 days ago</td>\n",
       "      <td>Over 200 applicants</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>With a startup spirit and 90,000+ curious and ...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3623351494/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Forecasting Analyst</td>\n",
       "      <td></td>\n",
       "      <td>NewAge Products Inc.</td>\n",
       "      <td>https://ca.linkedin.com/company/newage-product...</td>\n",
       "      <td>India</td>\n",
       "      <td>1 day ago</td>\n",
       "      <td>Over 200 applicants</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>NewAge Products Inc. (NAP), headquartered in N...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3737546187/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Internal Audit - Data Strategy - Analyst - Hyd...</td>\n",
       "      <td></td>\n",
       "      <td>Goldman Sachs</td>\n",
       "      <td>https://www.linkedin.com/company/goldman-sachs...</td>\n",
       "      <td>Hyderabad, Telangana, India</td>\n",
       "      <td>4 hours ago</td>\n",
       "      <td>Over 200 applicants</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>Job Description\\n\\nWhat We Do\\n\\nInternal Audi...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3648716404/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           job_title contact_person  \\\n",
       "0                     Analyst Data Science Analytics                  \n",
       "1                       Data Analyst (Mutual Series)                  \n",
       "2         Process Developer – Data Analyst-COR021664                  \n",
       "3                                Forecasting Analyst                  \n",
       "4  Internal Audit - Data Strategy - Analyst - Hyd...                  \n",
       "\n",
       "               company_name  \\\n",
       "0              ClickJobs.io   \n",
       "1  Franklin Templeton India   \n",
       "2                   Genpact   \n",
       "3      NewAge Products Inc.   \n",
       "4             Goldman Sachs   \n",
       "\n",
       "                                        company_link  \\\n",
       "0  https://uk.linkedin.com/company/clickjobsio?tr...   \n",
       "1  https://in.linkedin.com/company/franklintemple...   \n",
       "2  https://www.linkedin.com/company/genpact?trk=p...   \n",
       "3  https://ca.linkedin.com/company/newage-product...   \n",
       "4  https://www.linkedin.com/company/goldman-sachs...   \n",
       "\n",
       "                 location_name   post_dates     applicants_count  \\\n",
       "0  Bengaluru, Karnataka, India  5 hours ago       103 applicants   \n",
       "1   Mumbai, Maharashtra, India   2 days ago  Over 200 applicants   \n",
       "2  Noida, Uttar Pradesh, India   4 days ago  Over 200 applicants   \n",
       "3                        India    1 day ago  Over 200 applicants   \n",
       "4  Hyderabad, Telangana, India  4 hours ago  Over 200 applicants   \n",
       "\n",
       "  employment_type seniority_level  \\\n",
       "0       Full-time     Entry level   \n",
       "1       Full-time     Entry level   \n",
       "2       Full-time     Entry level   \n",
       "3       Full-time     Entry level   \n",
       "4       Full-time     Entry level   \n",
       "\n",
       "                                         description  \\\n",
       "0  Grow your career with a growing organization\\n...   \n",
       "1  At Franklin Templeton, everything we do is foc...   \n",
       "2  With a startup spirit and 90,000+ curious and ...   \n",
       "3  NewAge Products Inc. (NAP), headquartered in N...   \n",
       "4  Job Description\\n\\nWhat We Do\\n\\nInternal Audi...   \n",
       "\n",
       "                                           Job_links  \n",
       "0  https://www.linkedin.com/jobs/view/3738351411/...  \n",
       "1  https://www.linkedin.com/jobs/view/3737214595/...  \n",
       "2  https://www.linkedin.com/jobs/view/3623351494/...  \n",
       "3  https://www.linkedin.com/jobs/view/3737546187/...  \n",
       "4  https://www.linkedin.com/jobs/view/3648716404/...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'job_title':job_titles,'contact_person':contact_person,'company_name':company_name,'company_link':company_link,\n",
    "    'location_name':location_name,'post_dates':post_date,\n",
    "    'applicants_count':applicants_count,'employment_type':employmemt_type,\n",
    "    'seniority_level':seniority_level,'description':description,'Job_links':job_links\n",
    "})\n",
    "\n",
    "df.to_csv(r'D:\\scrapped_jobs\\Oct22_data_engineer_intern_intern_22ndOCT_22_linkedin_jobs.csv',index=False)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Entry level', 'Full-time', 'Engineering and Information Technology', 'Software Development']\n",
      "['Entry level', 'Full-time', 'Business Development and Sales', 'IT Services and IT Consulting']\n",
      "['Entry level', 'Full-time', 'Engineering and Information Technology', 'Software Development']\n",
      "Deepakarasi V Recruiter at Digispoc Technology\n",
      "['Full-time']\n"
     ]
    }
   ],
   "source": [
    "for link in [\"https://www.linkedin.com/jobs/view/3726846194/?eBP=CwEAAAGK6jL7xhZdwcUskrNit2JWpIJ2Mmq2mCZO0epVgihUEuR33E6Q6p6HXoSsBtwKTUbcBtZ9HqJx-Aij3gmIwhVkGDdAXz_4yYb8Yio9_dXBA72rXkSzDTC94l4_RSS2tazwUKOXrOK-5SifYc1T4lpolWpJWpHO1KwpN8o6y_PhJLUvAbxj8v-XJyEt7nPn9uibHMq3TjNXs0y4ZPGosd3Maj2UuQDCsV23fX0bDz4UU_61qeB26W0QRHRuZBtch4SoScJXVBF22WE98SltBZWyLf4GTd2Z6OaBlYMdOIkikRuSt1ChwbPmNU9FX7VDW0g3onMMzxsZwgnNLitvPd0nmX7u5NNHHw&refId=SVYUdRCkQgQn%2FNOOQhBdjg%3D%3D&trackingId=FIbt9FM3QSD8HpVkdRv74g%3D%3D&trk=flagship3_search_srp_jobs\",\n",
    "             'https://www.linkedin.com/jobs/view/3704252032/?eBP=CwEAAAGK6jL7xoWBvvbrxB-4IRsm2dnixY_DmZ-',\n",
    "             \"https://www.linkedin.com/jobs/view/3726846194/?eBP=CwEAAAGK6jL7xhZdwcUskrNit2JWpIJ2Mmq2mCZO0epVgihUEuR33E6Q6p6HXoSsBtwKTUbcBtZ9HqJx-Aij3gmIwhVkGDdAXz_4yYb8Yio9_dXBA72rXkSzDTC94l4_RSS2tazwUKOXrOK-5SifYc1T4lpolWpJWpHO1KwpN8o6y_PhJLUvAbxj8v-XJyEt7nPn9uibHMq3TjNXs0y4ZPGosd3Maj2UuQDCsV23fX0bDz4UU_61qeB26W0QRHRuZBtch4SoScJXVBF22WE98SltBZWyLf4GTd2Z6OaBlYMdOIkikRuSt1ChwbPmNU9FX7VDW0g3onMMzxsZwgnNLitvPd0nmX7u5NNHHw&refId=SVYUdRCkQgQn%2FNOOQhBdjg%3D%3D&trackingId=FIbt9FM3QSD8HpVkdRv74g%3D%3D&trk=flagship3_search_srp_jobs\",\n",
    "             \"https://www.linkedin.com/jobs/view/3726780795/?eBP=JOB_SEARCH_ORGANIC&refId=TrSZkvmL8BzLsdZxhyQd4g%3D%3D&trackingId=ChlB%2BWl5Ntwg4HvqNMmFYA%3D%3D&trk=flagship3_search_srp_jobs\"]:\n",
    "    \n",
    "    # op = webdriver.ChromeOptions()\n",
    "    # op.add_argument('headless')\n",
    "    # driver = webdriver.Chrome(options=op)\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(link)\n",
    "    driver.find_element(By.CLASS_NAME,\"show-more-less-html__button\").click()\n",
    "    driver.maximize_window()\n",
    "    \n",
    "    try:\n",
    "        person = driver.find_element(By.CSS_SELECTOR,\"div.message-the-recruiter>div>div>h3\").text\n",
    "        team = driver.find_element(By.CSS_SELECTOR,\"div.message-the-recruiter>div>div>h4\").text\n",
    "        print(person,team)\n",
    "        contact_person.append(person)\n",
    "        contact_team.append(team)\n",
    "    except:\n",
    "        contact_person.append('None')\n",
    "        contact_team.append('None')\n",
    "        \n",
    "\n",
    "    #top_card = driver.find_element(By.CLASS_NAME,'top-card-layout__card')\n",
    "\n",
    "    print([i.text for i in driver.find_elements(By.CSS_SELECTOR,\"span.description__job-criteria-text\")])\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging all the datasets and getting only contact\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "#Student Individual 2.1\n",
    "def read_file(): #reading files inside the folders\n",
    "    \n",
    "    csv_path = r'D:\\scrapped_jobs' #\n",
    "\n",
    "    # filenames = glob.glob(path + \"\\*.xlsx\")\n",
    "    path_filenames = glob. glob(csv_path + \"/*.csv\")\n",
    "    path_filenames.sort()\n",
    "#     print('File names:', csv_filenames)\n",
    "    \n",
    "    return path_filenames\n",
    "\n",
    "AllFiles=read_file()\n",
    "\n",
    "def file_names(path_filenames): #getting file names\n",
    "    files = []\n",
    "    \n",
    "    for index,file in enumerate(path_filenames):\n",
    "        f = file.split(\"D:\\\\scrapped_jobs\\\\\")[1].split(\".csv\")[0]\n",
    "        files.append(f'{f}')\n",
    "\n",
    "    return files\n",
    "\n",
    "files = file_names(read_file())\n",
    "\n",
    "merged_frame = pd.DataFrame({'job_title':[], 'contact_person':[], 'company_name':[],'Job_links':[],'scrap_date':[]})\n",
    "\n",
    "for i,j in zip(AllFiles,files):\n",
    "    \n",
    "    month_list = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n",
    "    \n",
    "    df = pd.read_csv(i) #creating dataframe by reading csv files\n",
    "    \n",
    "    #finding the scrap date and adding it to the dataframe\n",
    "    scrap_day = j.split('_')[0]\n",
    "    if scrap_day[0:3] in month_list:\n",
    "        scrap_date = datetime.datetime(2023, month_list.index(scrap_day[0:3])+1, int(scrap_day[3:]))\n",
    "        df['scrap_date'] = scrap_date\n",
    "        columns_to_take = ['job_title', 'contact_person', 'company_name','Job_links','scrap_date']\n",
    "    \n",
    "        try:\n",
    "            df = df.loc[:,columns_to_take]\n",
    "            bool_index = df.contact_person.apply(lambda x: type(x).__name__) != 'float'\n",
    "            index_list = [j for j,i in enumerate(bool_index) if i == True]\n",
    "            df = df.iloc[index_list,[0,1,2,3,4]]\n",
    "            merged_frame = pd.concat([merged_frame,df],ignore_index=True)\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "merged_frame.sort_values('scrap_date',inplace=True)\n",
    "merged_frame.to_csv(r'D:\\scrapped_jobs\\LinkedinContactsData.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
